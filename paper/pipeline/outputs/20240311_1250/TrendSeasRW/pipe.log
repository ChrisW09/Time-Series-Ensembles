
=========================================
== Pipeline Step 1: Data Preprocessing ==
=========================================

Searching time information...
Dates found in 'index' column!
Inferred frequency: month start
Data goes from 2004-01 to 2023-12, resulting in 240 observations.

Selecting target and covariates...
Target: TrendSeasRW
Covariates: None

Data Insights:
         TrendSeasRW
Date                
2004-01    69.091832
2004-02    68.281398
2004-03    59.419663
2004-04    60.921257
2004-05    57.008218

[Time elapsed: 20m 27s]


=====================================================
== Pipeline Step 2: Individual Models' Predictions ==
=====================================================

Splitting data for training of forecasters (train/test ratio: 30/70)...
Initial training set has 72 observations and goes from 2004-01 to 2009-12.

In an historical expanding window approach, there are 168 periods to be forecasted by the individual models: 2010-01 to 2023-12
Out-of-sample predictions are generated for next period: 2024-01

Now generating 168 one-step ahead historical expanding window predictions from model: Naive (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: AutoTheta (darts)
Now performing corresponding out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: AutoSARIMA (sktime)
Auto-fitting model. Refitting every 26th period.
...forecast 1 / 168 done
...forecast 42 / 168 done
...forecast 84 / 168 done
...forecast 126 / 168 done
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: Exponential Smoothing (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: TiDE (darts)
Train dataset contains 60 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 60 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 72 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 84 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 96 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 108 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 120 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 132 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 144 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 156 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 168 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 180 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 192 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 204 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 216 samples.
Time series values are 64-bits; casting model to float64.
Now performing corresponding out-of-sample predictions...
Train dataset contains 228 samples.
Attempting to retrain/fine-tune the model without resuming from a checkpoint. This is currently discouraged. Consider model `TiDEModel.load_weights()` to load the weights for fine-tuning.
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: STL (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: XGBoost (darts)
Now performing corresponding out-of-sample predictions...
...finished!

Skipping covariate forecasters since no covariates are given.

Finished predictions of individual forecasters!

Insights into forecasters' historical predictions:
             Naive  AutoTheta  AutoSARIMA  Exponential Smoothing       TiDE  \
Date                                                                          
2010-01  55.335626  52.895718   53.638535              53.600755  49.534512   
2010-02  54.648712  54.019543   55.275822              56.320104  49.730125   
2010-03  55.429574  54.925930   54.193576              52.625544  50.000986   
2010-04  57.888055  56.832504   56.457815              54.435895  50.547302   
2010-05  53.278633  54.588055   53.738130              52.284274  51.587000   

               STL    XGBoost  
Date                           
2010-01  55.354353  52.346306  
2010-02  60.088353  49.883060  
2010-03  53.127261  53.310276  
2010-04  51.757218  51.985195  
2010-05  50.102725  52.860527  

Insights into forecasters' future predictions:
            Naive  AutoTheta  AutoSARIMA  Exponential Smoothing       TiDE  \
Date                                                                         
2024-01  81.82247  81.454099   81.111836              81.328604  80.229994   

               STL    XGBoost  
Date                           
2024-01  83.627385  82.093315  

[Time elapsed: 26m 57s]


===================================================
== Pipeline Step 3: Ensemble Models' Predictions ==
===================================================

Splitting individual forecast data (n = 168) for training of ensemblers (train/test ratio: 25/75)...
Initial training set has 42 observations and goes from 2010-01 to 2013-06

In an historical expanding window approach, there are 126 periods to be forecasted by the ensemble models: 2013-07 to 2023-12
Out-of-sample predictions are generated for next period: 2023-12-31 00:00:00

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Simple'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse RMSE'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse Variance'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse Error Covariance'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Meta - SVR (sklearn)'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Meta - Random Forest (sklearn)'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Finished predictions of ensemble forecasters!

Insights into ensemblers' historical predictions:
         Weighted Ensemble: Simple  Weighted Ensemble: Inverse RMSE  \
2013-07                  66.845784                        66.964711   
2013-08                  63.671729                        63.678319   
2013-09                  63.718082                        63.414553   
2013-10                  60.811444                        60.932045   
2013-11                  61.649617                        61.635133   

         Weighted Ensemble: Inverse Variance  \
2013-07                            66.956215   
2013-08                            63.672342   
2013-09                            63.793592   
2013-10                            60.749777   
2013-11                            61.524210   

         Weighted Ensemble: Inverse Error Covariance  Meta Ensemble: SVR  \
2013-07                                    67.664392           67.379153   
2013-08                                    63.060972           63.844880   
2013-09                                    58.680009           62.095476   
2013-10                                    62.396354           63.791636   
2013-11                                    64.026134           65.634487   

         Meta Ensemble: Random Forest  
2013-07                     68.336017  
2013-08                     60.676208  
2013-09                     61.260321  
2013-10                     61.624103  
2013-11                     64.121364  

Insights into ensemblers' future predictions:
         Weighted Ensemble: Simple  Weighted Ensemble: Inverse RMSE  \
Date                                                                  
2024-01                  81.666816                         81.61893   

         Weighted Ensemble: Inverse Variance  \
Date                                           
2024-01                            81.640771   

         Weighted Ensemble: Inverse Error Covariance  Meta Ensemble: SVR  \
Date                                                                       
2024-01                                    82.043979           81.676472   

         Meta Ensemble: Random Forest  
Date                                   
2024-01                     80.332474  

Merging...
...finished!


[Time elapsed: 27m 26s]


==============================================================
== Pipeline Step 4: Ranking Models' Predictive Performance ==
==============================================================

Calculating MAPE, RMSE, sMAPE per model...
Ranking models ...
...finished!

Results:
                                                 MAPE      RMSE     sMAPE  \
Model                                                                       
Weighted Ensemble: Inverse Variance          0.034969  3.142844  2.330395   
Weighted Ensemble: Inverse RMSE              0.035057  3.150374  2.336016   
Weighted Ensemble: Simple                    0.035058  3.151048  2.336445   
Weighted Ensemble: Inverse Error Covariance  0.037188  3.318728  2.476791   
Exponential Smoothing                        0.037530  3.340527  2.498175   
AutoSARIMA                                   0.037537  3.244438  2.497060   
AutoTheta                                    0.038564  3.405900  2.566496   
TiDE                                         0.038783  3.555382  2.585410   
Meta Ensemble: SVR                           0.038787  3.435591  2.588056   
Meta Ensemble: Random Forest                 0.040790  3.643726  2.722857   
Naive                                        0.040938  3.502125  2.724533   
XGBoost                                      0.044369  4.102619  2.976202   
STL                                          0.049250  4.413122  3.273544   

                                             MAPE Ranking  RMSE Ranking  \
Model                                                                     
Weighted Ensemble: Inverse Variance                     1             1   
Weighted Ensemble: Inverse RMSE                         2             2   
Weighted Ensemble: Simple                               3             3   
Weighted Ensemble: Inverse Error Covariance             4             5   
Exponential Smoothing                                   5             6   
AutoSARIMA                                              6             4   
AutoTheta                                               7             7   
TiDE                                                    8            10   
Meta Ensemble: SVR                                      9             8   
Meta Ensemble: Random Forest                           10            11   
Naive                                                  11             9   
XGBoost                                                12            12   
STL                                                    13            13   

                                             sMAPE Ranking  
Model                                                       
Weighted Ensemble: Inverse Variance                      1  
Weighted Ensemble: Inverse RMSE                          2  
Weighted Ensemble: Simple                                3  
Weighted Ensemble: Inverse Error Covariance              4  
Exponential Smoothing                                    6  
AutoSARIMA                                               5  
AutoTheta                                                7  
TiDE                                                     8  
Meta Ensemble: SVR                                       9  
Meta Ensemble: Random Forest                            10  
Naive                                                   11  
XGBoost                                                 12  
STL                                                     13  

The 'Weighted Ensemble: Inverse Variance' is identified as the best model based on the MAPE value of its the historical predictions.
Thus, it is recommended to work with the future predictions coming from this model:
Date
2024-01    81.640771
Freq: M, Name: Weighted Ensemble: Inverse Variance, dtype: float64

[2024-03-11 13:17] Finished Pipeline for TrendSeasRW dataset!
[Total time elapsed: 27m 26s]
=================================================================================

=================================================================================
[2024-03-11 12:50] Starting  Pipeline for WeakSARIMA dataset...

=========================================
== Pipeline Step 1: Data Preprocessing ==
=========================================

Searching time information...
Dates found in 'index' column!
Inferred frequency: month start
Data goes from 2004-01 to 2023-12, resulting in 240 observations.

Selecting target and covariates...
Target: WeakSARIMA
Covariates: None

Data Insights:
         WeakSARIMA
Date               
2004-01   67.834583
2004-02   67.990248
2004-03   60.532925
2004-04   60.827498
2004-05   57.793300

[Time elapsed: 27m 26s]


=====================================================
== Pipeline Step 2: Individual Models' Predictions ==
=====================================================

Splitting data for training of forecasters (train/test ratio: 30/70)...
Initial training set has 72 observations and goes from 2004-01 to 2009-12.

In an historical expanding window approach, there are 168 periods to be forecasted by the individual models: 2010-01 to 2023-12
Out-of-sample predictions are generated for next period: 2024-01

Now generating 168 one-step ahead historical expanding window predictions from model: Naive (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: AutoTheta (darts)
Now performing corresponding out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: AutoSARIMA (sktime)
Auto-fitting model. Refitting every 26th period.
...forecast 1 / 168 done
...forecast 42 / 168 done
...forecast 84 / 168 done
...forecast 126 / 168 done
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: Exponential Smoothing (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: TiDE (darts)
Train dataset contains 60 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 60 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 72 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 84 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 96 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 108 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 120 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 132 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 144 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 156 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 168 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 180 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 192 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 204 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 216 samples.
Time series values are 64-bits; casting model to float64.
Now performing corresponding out-of-sample predictions...
Train dataset contains 228 samples.
Attempting to retrain/fine-tune the model without resuming from a checkpoint. This is currently discouraged. Consider model `TiDEModel.load_weights()` to load the weights for fine-tuning.
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: STL (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: XGBoost (darts)
Now performing corresponding out-of-sample predictions...
...finished!

Skipping covariate forecasters since no covariates are given.

Finished predictions of individual forecasters!

Insights into forecasters' historical predictions:
             Naive  AutoTheta  AutoSARIMA  Exponential Smoothing       TiDE  \
Date                                                                          
2010-01  54.125598  52.793167   53.419395              53.209749  49.240113   
2010-02  54.391598  54.024696   55.400318              56.448705  49.518899   
2010-03  55.414677  55.067646   54.473555              53.331175  49.765609   
2010-04  57.505825  57.061562   56.715515              54.953830  50.406719   
2010-05  54.400890  54.757545   54.307201              53.039425  51.427948   

               STL    XGBoost  
Date                           
2010-01  54.909797  53.260006  
2010-02  59.503070  49.700474  
2010-03  54.191112  53.145664  
2010-04  52.546657  56.468643  
2010-05  50.757380  49.862255  

Insights into forecasters' future predictions:
           Naive  AutoTheta  AutoSARIMA  Exponential Smoothing      TiDE  \
Date                                                                       
2024-01  80.9192  81.555561   81.260665              81.339129  79.37826   

               STL    XGBoost  
Date                           
2024-01  83.754654  82.660751  

[Time elapsed: 34m 07s]


===================================================
== Pipeline Step 3: Ensemble Models' Predictions ==
===================================================

Splitting individual forecast data (n = 168) for training of ensemblers (train/test ratio: 25/75)...
Initial training set has 42 observations and goes from 2010-01 to 2013-06

In an historical expanding window approach, there are 126 periods to be forecasted by the ensemble models: 2013-07 to 2023-12
Out-of-sample predictions are generated for next period: 2023-12-31 00:00:00

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Simple'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse RMSE'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse Variance'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse Error Covariance'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Meta - SVR (sklearn)'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Meta - Random Forest (sklearn)'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Finished predictions of ensemble forecasters!

Insights into ensemblers' historical predictions:
         Weighted Ensemble: Simple  Weighted Ensemble: Inverse RMSE  \
2013-07                  67.395399                        67.500891   
2013-08                  64.461483                        64.531736   
2013-09                  64.594833                        64.378904   
2013-10                  61.223232                        61.268124   
2013-11                  61.213639                        61.219138   

         Weighted Ensemble: Inverse Variance  \
2013-07                            67.454962   
2013-08                            64.452529   
2013-09                            64.632941   
2013-10                            61.128120   
2013-11                            61.061316   

         Weighted Ensemble: Inverse Error Covariance  Meta Ensemble: SVR  \
2013-07                                    68.633218           68.122943   
2013-08                                    64.739839           65.262397   
2013-09                                    59.370727           65.406900   
2013-10                                    62.463012           63.225965   
2013-11                                    63.927381           65.182821   

         Meta Ensemble: Random Forest  
2013-07                     68.095096  
2013-08                     64.066544  
2013-09                     61.320915  
2013-10                     63.105456  
2013-11                     60.898983  

Insights into ensemblers' future predictions:
         Weighted Ensemble: Simple  Weighted Ensemble: Inverse RMSE  \
Date                                                                  
2024-01                  81.552604                        81.486242   

         Weighted Ensemble: Inverse Variance  \
Date                                           
2024-01                            81.541495   

         Weighted Ensemble: Inverse Error Covariance  Meta Ensemble: SVR  \
Date                                                                       
2024-01                                    81.579908           81.133036   

         Meta Ensemble: Random Forest  
Date                                   
2024-01                     81.611616  

Merging...
...finished!


[Time elapsed: 34m 36s]


==============================================================
== Pipeline Step 4: Ranking Models' Predictive Performance ==
==============================================================

Calculating MAPE, RMSE, sMAPE per model...
Ranking models ...
...finished!

Results:
                                                 MAPE      RMSE     sMAPE  \
Model                                                                       
Weighted Ensemble: Inverse Variance          0.028514  2.577474  1.901336   
Weighted Ensemble: Simple                    0.028550  2.590860  1.903842   
Weighted Ensemble: Inverse RMSE              0.028590  2.585070  1.906166   
AutoSARIMA                                   0.030213  2.638002  2.011089   
Exponential Smoothing                        0.030866  2.743687  2.056202   
Weighted Ensemble: Inverse Error Covariance  0.030954  2.763787  2.062406   
AutoTheta                                    0.032079  2.822191  2.136678   
Meta Ensemble: SVR                           0.032303  2.927370  2.156422   
Meta Ensemble: Random Forest                 0.032714  2.917809  2.186314   
Naive                                        0.032951  2.791199  2.194743   
TiDE                                         0.033343  3.063924  2.221320   
XGBoost                                      0.034701  3.180757  2.328224   
STL                                          0.044823  3.999710  2.979882   

                                             MAPE Ranking  RMSE Ranking  \
Model                                                                     
Weighted Ensemble: Inverse Variance                     1             1   
Weighted Ensemble: Simple                               2             3   
Weighted Ensemble: Inverse RMSE                         3             2   
AutoSARIMA                                              4             4   
Exponential Smoothing                                   5             5   
Weighted Ensemble: Inverse Error Covariance             6             6   
AutoTheta                                               7             8   
Meta Ensemble: SVR                                      8            10   
Meta Ensemble: Random Forest                            9             9   
Naive                                                  10             7   
TiDE                                                   11            11   
XGBoost                                                12            12   
STL                                                    13            13   

                                             sMAPE Ranking  
Model                                                       
Weighted Ensemble: Inverse Variance                      1  
Weighted Ensemble: Simple                                2  
Weighted Ensemble: Inverse RMSE                          3  
AutoSARIMA                                               4  
Exponential Smoothing                                    5  
Weighted Ensemble: Inverse Error Covariance              6  
AutoTheta                                                7  
Meta Ensemble: SVR                                       8  
Meta Ensemble: Random Forest                             9  
Naive                                                   10  
TiDE                                                    11  
XGBoost                                                 12  
STL                                                     13  

The 'Weighted Ensemble: Inverse Variance' is identified as the best model based on the MAPE value of its the historical predictions.
Thus, it is recommended to work with the future predictions coming from this model:
Date
2024-01    81.541495
Freq: M, Name: Weighted Ensemble: Inverse Variance, dtype: float64

[2024-03-11 13:24] Finished Pipeline for WeakSARIMA dataset!
[Total time elapsed: 34m 36s]
=================================================================================

