
=========================================
== Pipeline Step 1: Data Preprocessing ==
=========================================

Searching time information...
Dates found in 'index' column!
Inferred frequency: month start
Data goes from 2004-01 to 2023-12, resulting in 240 observations.

Selecting target and covariates...
Target: WeakSARIMA
Covariates: None

Data Insights:
         WeakSARIMA
Date               
2004-01   67.834583
2004-02   67.990248
2004-03   60.532925
2004-04   60.827498
2004-05   57.793300

[Time elapsed: 27m 26s]


=====================================================
== Pipeline Step 2: Individual Models' Predictions ==
=====================================================

Splitting data for training of forecasters (train/test ratio: 30/70)...
Initial training set has 72 observations and goes from 2004-01 to 2009-12.

In an historical expanding window approach, there are 168 periods to be forecasted by the individual models: 2010-01 to 2023-12
Out-of-sample predictions are generated for next period: 2024-01

Now generating 168 one-step ahead historical expanding window predictions from model: Naive (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: AutoTheta (darts)
Now performing corresponding out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: AutoSARIMA (sktime)
Auto-fitting model. Refitting every 26th period.
...forecast 1 / 168 done
...forecast 42 / 168 done
...forecast 84 / 168 done
...forecast 126 / 168 done
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: Exponential Smoothing (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: TiDE (darts)
Train dataset contains 60 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 60 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 72 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 84 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 96 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 108 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 120 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 132 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 144 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 156 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 168 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 180 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 192 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 204 samples.
Time series values are 64-bits; casting model to float64.
Train dataset contains 216 samples.
Time series values are 64-bits; casting model to float64.
Now performing corresponding out-of-sample predictions...
Train dataset contains 228 samples.
Attempting to retrain/fine-tune the model without resuming from a checkpoint. This is currently discouraged. Consider model `TiDEModel.load_weights()` to load the weights for fine-tuning.
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: STL (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 168 one-step ahead historical expanding window predictions from model: XGBoost (darts)
Now performing corresponding out-of-sample predictions...
...finished!

Skipping covariate forecasters since no covariates are given.

Finished predictions of individual forecasters!

Insights into forecasters' historical predictions:
             Naive  AutoTheta  AutoSARIMA  Exponential Smoothing       TiDE  \
Date                                                                          
2010-01  54.125598  52.793167   53.419395              53.209749  49.240113   
2010-02  54.391598  54.024696   55.400318              56.448705  49.518899   
2010-03  55.414677  55.067646   54.473555              53.331175  49.765609   
2010-04  57.505825  57.061562   56.715515              54.953830  50.406719   
2010-05  54.400890  54.757545   54.307201              53.039425  51.427948   

               STL    XGBoost  
Date                           
2010-01  54.909797  53.260006  
2010-02  59.503070  49.700474  
2010-03  54.191112  53.145664  
2010-04  52.546657  56.468643  
2010-05  50.757380  49.862255  

Insights into forecasters' future predictions:
           Naive  AutoTheta  AutoSARIMA  Exponential Smoothing      TiDE  \
Date                                                                       
2024-01  80.9192  81.555561   81.260665              81.339129  79.37826   

               STL    XGBoost  
Date                           
2024-01  83.754654  82.660751  

[Time elapsed: 34m 07s]


===================================================
== Pipeline Step 3: Ensemble Models' Predictions ==
===================================================

Splitting individual forecast data (n = 168) for training of ensemblers (train/test ratio: 25/75)...
Initial training set has 42 observations and goes from 2010-01 to 2013-06

In an historical expanding window approach, there are 126 periods to be forecasted by the ensemble models: 2013-07 to 2023-12
Out-of-sample predictions are generated for next period: 2023-12-31 00:00:00

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Simple'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse RMSE'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse Variance'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Inverse Error Covariance'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Meta - SVR (sklearn)'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Now generating 126 one-step ahead historical expanding window predictions from ensemble model: 'Meta - Random Forest (sklearn)'
...Forecast 1 / 126 done
...Forecast 32 / 126 done
...Forecast 63 / 126 done
...Forecast 95 / 126 done
...finished!
Performing out-of-sample predictions...
...finished!

Finished predictions of ensemble forecasters!

Insights into ensemblers' historical predictions:
         Weighted Ensemble: Simple  Weighted Ensemble: Inverse RMSE  \
2013-07                  67.395399                        67.500891   
2013-08                  64.461483                        64.531736   
2013-09                  64.594833                        64.378904   
2013-10                  61.223232                        61.268124   
2013-11                  61.213639                        61.219138   

         Weighted Ensemble: Inverse Variance  \
2013-07                            67.454962   
2013-08                            64.452529   
2013-09                            64.632941   
2013-10                            61.128120   
2013-11                            61.061316   

         Weighted Ensemble: Inverse Error Covariance  Meta Ensemble: SVR  \
2013-07                                    68.633218           68.122943   
2013-08                                    64.739839           65.262397   
2013-09                                    59.370727           65.406900   
2013-10                                    62.463012           63.225965   
2013-11                                    63.927381           65.182821   

         Meta Ensemble: Random Forest  
2013-07                     68.095096  
2013-08                     64.066544  
2013-09                     61.320915  
2013-10                     63.105456  
2013-11                     60.898983  

Insights into ensemblers' future predictions:
         Weighted Ensemble: Simple  Weighted Ensemble: Inverse RMSE  \
Date                                                                  
2024-01                  81.552604                        81.486242   

         Weighted Ensemble: Inverse Variance  \
Date                                           
2024-01                            81.541495   

         Weighted Ensemble: Inverse Error Covariance  Meta Ensemble: SVR  \
Date                                                                       
2024-01                                    81.579908           81.133036   

         Meta Ensemble: Random Forest  
Date                                   
2024-01                     81.611616  

Merging...
...finished!


[Time elapsed: 34m 36s]


==============================================================
== Pipeline Step 4: Ranking Models' Predictive Performance ==
==============================================================

Calculating MAPE, RMSE, sMAPE per model...
Ranking models ...
...finished!

Results:
                                                 MAPE      RMSE     sMAPE  \
Model                                                                       
Weighted Ensemble: Inverse Variance          0.028514  2.577474  1.901336   
Weighted Ensemble: Simple                    0.028550  2.590860  1.903842   
Weighted Ensemble: Inverse RMSE              0.028590  2.585070  1.906166   
AutoSARIMA                                   0.030213  2.638002  2.011089   
Exponential Smoothing                        0.030866  2.743687  2.056202   
Weighted Ensemble: Inverse Error Covariance  0.030954  2.763787  2.062406   
AutoTheta                                    0.032079  2.822191  2.136678   
Meta Ensemble: SVR                           0.032303  2.927370  2.156422   
Meta Ensemble: Random Forest                 0.032714  2.917809  2.186314   
Naive                                        0.032951  2.791199  2.194743   
TiDE                                         0.033343  3.063924  2.221320   
XGBoost                                      0.034701  3.180757  2.328224   
STL                                          0.044823  3.999710  2.979882   

                                             MAPE Ranking  RMSE Ranking  \
Model                                                                     
Weighted Ensemble: Inverse Variance                     1             1   
Weighted Ensemble: Simple                               2             3   
Weighted Ensemble: Inverse RMSE                         3             2   
AutoSARIMA                                              4             4   
Exponential Smoothing                                   5             5   
Weighted Ensemble: Inverse Error Covariance             6             6   
AutoTheta                                               7             8   
Meta Ensemble: SVR                                      8            10   
Meta Ensemble: Random Forest                            9             9   
Naive                                                  10             7   
TiDE                                                   11            11   
XGBoost                                                12            12   
STL                                                    13            13   

                                             sMAPE Ranking  
Model                                                       
Weighted Ensemble: Inverse Variance                      1  
Weighted Ensemble: Simple                                2  
Weighted Ensemble: Inverse RMSE                          3  
AutoSARIMA                                               4  
Exponential Smoothing                                    5  
Weighted Ensemble: Inverse Error Covariance              6  
AutoTheta                                                7  
Meta Ensemble: SVR                                       8  
Meta Ensemble: Random Forest                             9  
Naive                                                   10  
TiDE                                                    11  
XGBoost                                                 12  
STL                                                     13  

The 'Weighted Ensemble: Inverse Variance' is identified as the best model based on the MAPE value of its the historical predictions.
Thus, it is recommended to work with the future predictions coming from this model:
Date
2024-01    81.541495
Freq: M, Name: Weighted Ensemble: Inverse Variance, dtype: float64

[2024-03-11 13:24] Finished Pipeline for WeakSARIMA dataset!
[Total time elapsed: 34m 36s]
=================================================================================

