=================================================================================
[2024-03-08 09:50] Starting  Pipeline...

=========================================
== Pipeline Step 1: Data Preprocessing ==
=========================================

Searching time information...
Dates found in 'index' column!
Inferred frequency: month start
Data goes from 2004-01 to 2015-12, resulting in 144 observations.

Selecting target and covariates...
Target: y
Covariates: x1, x2, x3

Data Insights:
                 y          x1         x2          x3
Date                                                 
2004-01  50.840469  427.595799  55.337904  900.325291
2004-02  52.871538  434.062163  54.959155  900.775888
2004-03  53.769316  453.264284  56.470633  899.510058
2004-04  57.672973  459.367523  56.704233  903.524834
2004-05  57.182051  462.354356  61.557907  905.071762

[Time elapsed: 00s]


=====================================================
== Pipeline Step 2: Individual Models' Predictions ==
=====================================================

Splitting data for training of forecasters (train/test ratio: 30/70)...
Initial training set has 44 observations and goes from 2004-01 to 2007-08.

In an historical expanding window approach, there are 100 periods to be forecasted by the individual models: 2007-09 to 2015-12
Out-of-sample predictions are generated for next period: 2016-01

Now generating 100 one-step ahead historical expanding window predictions from model: Naive (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 100 one-step ahead historical expanding window predictions from model: STL (sktime)
Performing out-of-sample predictions...
...finished!

Now generating 100 one-step ahead historical expanding window predictions from model: XGBoost (darts)
Now performing corresponding out-of-sample predictions...
...finished!
Skipping with_covariates since no models are provided.

Finished predictions of individual forecasters!

Insights into forecasters' historical predictions:
             Naive        STL    XGBoost
Date                                    
2007-09  39.396929  45.096682  40.291088
2007-10  43.742069  42.109471  41.772690
2007-11  43.020168  45.768581  41.873455
2007-12  47.340548  44.613499  46.844360
2008-01  48.280066  42.637916  49.353115

Insights into forecasters' future predictions:
             Naive        STL    XGBoost
Date                                    
2016-01  84.936483  91.278306  84.968803

[Time elapsed: 44s]


===================================================
== Pipeline Step 3: Ensemble Models' Predictions ==
===================================================

Splitting individual forecast data (n = 100) for training of ensemblers (train/test ratio: 25/75)...
Initial training set has 25 observations and goes from 2007-09 to 2009-09

In an historical expanding window approach, there are 75 periods to be forecasted by the ensemble models: 2009-10 to 2015-12
Out-of-sample predictions are generated for next period: 2015-12-31 00:00:00

Now generating 75 one-step ahead historical expanding window predictions from ensemble model: 'Weighted - Simple'
...Forecast 1 / 75
...Forecast 19 / 75
...Forecast 38 / 75
...Forecast 57 / 75
...finished!
Performing out-of-sample predictions...
...finished!

Finished predictions of ensemble forecasters!

Insights into ensemblers' historical predictions:
         Weighted Ensemble: Simple
2009-10                  60.000553
2009-11                  61.120487
2009-12                  62.143678
2010-01                  61.011465
2010-02                  61.240424

Insights into ensemblers' future predictions:
         Weighted Ensemble: Simple
Date                              
2016-01                  87.061198

Merging...
...finished!


Exporting historical predictions and future predictions as csv to C:\Users\Work\OneDrive\GAU\3. Semester\Statistisches Praktikum\Git\NEW_Ensemble_Techniques_TS_FC\user\outputs\noisy_simdata\20240308_0950...
...finished!

[Time elapsed: 45s]


==============================================================
== Pipeline Step 4: Ranking Models' Predictive Performance ==
==============================================================

Calculating RMSE, MAPE, sMAPE per model...
Ranking models ...
...finished!

Results:
                               RMSE      MAPE     sMAPE  RMSE Ranking  \
Model                                                                   
Naive                      2.883142  0.029242  1.957764             1   
Weighted Ensemble: Simple  3.121506  0.030401  2.042777             2   
XGBoost                    3.342290  0.032360  2.187298             3   
STL                        4.901734  0.050245  3.360780             4   

                           MAPE Ranking  sMAPE Ranking  
Model                                                   
Naive                                 1              1  
Weighted Ensemble: Simple             2              2  
XGBoost                               3              3  
STL                                   4              4  

The 'Naive' is identified as the best model based on the MAPE value of its the historical predictions.
Thus, it is recommended to work with the future predictions coming from this model:
Date
2016-01    84.936483
Freq: M, Name: Naive, dtype: float64

Exporting metrics_ranking as csv to C:\Users\Work\OneDrive\GAU\3. Semester\Statistisches Praktikum\Git\NEW_Ensemble_Techniques_TS_FC\user\outputs\noisy_simdata\20240308_0950...
...finished!

[2024-03-08 09:50] Finished Pipeline!
[Total time elapsed: 45s]
=================================================================================
